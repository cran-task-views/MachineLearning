<CRANTaskView>

  <name>MachineLearning</name>
  <topic>Machine Learning &amp; Statistical Learning</topic>
  <maintainer email="Torsten.Hothorn@R-project.org">Torsten Hothorn</maintainer>
  <version>2009-01-24</version>
  
  <info>
    Several add-on packages implement ideas and methods developed at the
    borderline between computer science and statistics - this field of research
    is usually referred to as machine learning. 

    The packages can be roughly structured into the following topics:
    <ul>
      <li><i>Neural Networks</i>: Single-hidden-layer neural network are 
             implemented in package nnet as part of the <pkg>VR</pkg>
             bundle (shipped with base R). </li>
      <li><i>Recursive Partitioning</i>: Tree-structured models for
             regression, classification and survival analysis, following the
	     ideas in the CART book, are
             implemented in <pkg>rpart</pkg> (shipped with base R) and <pkg>tree</pkg>.
             Package <pkg>rpart</pkg> is recommended for computing CART-like
             trees. 
             A rich toolbox of partitioning algorithms is available in
             <A HREF="http://www.cs.waikato.ac.nz/~ml/weka/">Weka</A>, 
             package <pkg>RWeka</pkg> provides an interface to this
             implementation, including the J4.8-variant of C4.5 and M5.
             <br/>
             Two recursive partitioning algorithms with unbiased variable
             selection and statistical stopping criterion are implemented in 
             package <pkg>party</pkg>. Function <code>ctree()</code> is based on 
             non-parametrical conditional inference procedures for testing 
             independence between response and each input variable whereas
             <code>mob()</code> can be used to partition parametric models.
             Extensible tools for visualizing binary trees
             and node distributions of the response are available in package
             <pkg>party</pkg> as well.
             <br/>
             An adaptation of <pkg>rpart</pkg> for multivariate responses
             is available in package <pkg>mvpart</pkg>. A tree algorithm fitting 
             nearest neighbors in each node is implemented in package 
             <pkg>knnTree</pkg>. For problems with binary input variables
             the package <pkg>LogicReg</pkg> implements logic regression.
             Graphical tools for the visualization of
             trees are available in packages <pkg>maptree</pkg> and 
             <pkg>pinktoe</pkg>. </li>
      <li><i>Random Forests</i>: The reference implementation of the random
             forest algorithm for regression and classification is available in 
             package <pkg>randomForest</pkg>. Package <pkg>ipred</pkg> has bagging
             for regression, classification and survival analysis as well as
             bundling, a combination of multiple models via
             ensemble learning. In addition, a random forest variant for
             response variables measured at arbitrary scales based on
             conditional inference trees is implemented in package <pkg>party</pkg>.
             <pkg>randomSurvivalForest</pkg> offers a random forest algorithm for
             censored data.
             The <pkg>varSelRF</pkg> package focuses on variable selection by means
             for random forest algorithms.</li>
      <li><i>Regularized and Shrinkage Methods</i>: Regression models with some
             constraint on the parameter estimates can be fitted with the
             <pkg>lasso2</pkg> and <pkg>lars</pkg> packages. Lasso with
             simultaneous updates for groups of parameters (groupwise lasso) 
             is available in package <pkg>grplasso</pkg>.
             The L1 regularization path for generalized linear models and
             Cox models can be obtained from functions available in package 
             <pkg>glmpath</pkg>, the entire lasso or elastic-net regularization path (also in <pkg>elasticnet</pkg>)
             for linear regression, 
             logistic and multinomial regression models can be obtained from package <pkg>glmnet</pkg>.
             The <pkg>penalized</pkg> package provides
             an alternative implementation of lasso (L1) and ridge (L2) 
             penalized regression models (both GLM and Cox models).
             A generalisation of the Lasso shrinkage technique for linear regression
             is called relaxed lasso and is available in package <pkg>relaxo</pkg>.
             The shrunken
             centroids classifier and utilities for gene expression analyses are
             implemented in package <pkg>pamr</pkg>. An implementation
             of multivariate adaptive regression splines is available
             in package <pkg>earth</pkg>.</li> 
      <li><i>Boosting</i>: Various forms of gradient boosting are
             implemented in packages <pkg>gbm</pkg> (tree-based functional gradient
             descent boosting) and <pkg>boost</pkg> (including LogitBoost
             and L2Boost). Package <pkg>GAMBoost</pkg> can be used to fit generalized additive models
             by a boosting algorithm. An extensible boosting framework for
             generalized linear, additive and nonparametric models is available in
             package <pkg>mboost</pkg>. Likelihood-based boosting for Cox models
             is implemented in <pkg>CoxBoost</pkg></li>
      <li><i>Support Vector Machines and Kernel Methods</i>: The function <code>svm()</code> from 
             <pkg>e1071</pkg> offers an interface to the LIBSVM library and
             package <pkg>kernlab</pkg> implements a flexible framework 
             for kernel learning (including SVMs, RVMs and other kernel
	     learning algorithms). An interface to the SVMlight implementation
	     (only for one-against-all classification) is provided in package
	     <pkg>klaR</pkg>.
             The relevant dimension in kernel feature spaces can be estimated
             using <pkg>rdetools</pkg> which also offers procedures for model selection
             and prediction.</li>
      <li><i>Bayesian Methods</i>: Bayesian Additive Regression Trees (BART),
             where the final model is defined in terms of the sum over
             many weak learners (not unlike ensemble methods), 
             are implemented in package <pkg>BayesTree</pkg>. 
             Bayesian nonstationary, semiparametric nonlinear regression 
             and design by treed Gaussian processes including Bayesian CART and 
             treed linear models are made available by package <pkg>tgp</pkg>.
             Bayesian logistic regression models that consider the high-order interactions
             are available from package <pkg>BPHO</pkg> and Bayesian naive Bayes models 
             for binary classification with bias corrected feature selection is implemented in
             package <pkg>predbayescor</pkg>.</li>
      <li><i>Optimization using Genetic Algorithms</i> Packages <pkg>gafit</pkg> and
             <pkg>rgenoud</pkg> offer optimization routines based on genetic algorithms. </li>
      <li><i>Association Rules</i>: Package
             <pkg>arules</pkg> provides both data structures for efficient
             handling of sparse binary data as well as interfaces to
             implementations of Apriori and Eclat for mining
             frequent itemsets, maximal frequent itemsets, closed 
             frequent itemsets and association rules.</li>
      <li><i>Model selection and validation</i>: Package <pkg>e1071</pkg>
             has function <code>tune()</code> for hyper parameter tuning and 
             function <code>errorest()</code> (<pkg>ipred</pkg>) can be used for
             error rate estimation. The cost parameter C for support vector
             machines can be chosen utilizing the functionality of package
             <pkg>svmpath</pkg>.
             Functions for ROC analysis and other visualisation techniques 
             for comparing candidate classifiers are available from package 
             <pkg>ROCR</pkg>.
             Package <pkg>caret</pkg> provides miscellaneous functions 
             for building predictive models, including parameter tuning 
             and  variable importance measures. The package can be used 
             with various parallel implementations (e.g. MPI, NWS etc).</li>
      <li><i>Elements of Statistical Learning</i>: Data sets, functions and
             examples from the book 
             <A HREF="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning: Data Mining,
             Inference, and Prediction</A> by Trevor Hastie, Robert Tibshirani and 
             Jerome Friedman have been packaged and are available as
             <pkg>ElemStatLearn</pkg>.</li>
    </ul>
  </info>

  <packagelist>
    <pkg>arules</pkg>
    <pkg>BayesTree</pkg>
    <pkg>boost</pkg>
    <pkg>BPHO</pkg>
    <pkg>caret</pkg>
    <pkg>CoxBoost</pkg>
    <pkg priority="core">e1071</pkg>
    <pkg>ElemStatLearn</pkg>
    <pkg>earth</pkg>
    <pkg>elasticnet</pkg>
    <pkg>gafit</pkg>
    <pkg>GAMBoost</pkg>
    <pkg priority="core">gbm</pkg>
    <pkg>glmnet</pkg>
    <pkg>glmpath</pkg>
    <pkg>grplasso</pkg>
    <pkg>ipred</pkg>
    <pkg priority="core">kernlab</pkg>
    <pkg>klaR</pkg>
    <pkg>lars</pkg>
    <pkg>lasso2</pkg>
    <pkg priority = "core">mboost</pkg>
    <pkg>mvpart</pkg>
    <pkg>pamr</pkg>
    <pkg>party</pkg>
    <pkg>penalized</pkg>
    <pkg>predbayescor</pkg>
    <pkg>ROCR</pkg>
    <pkg priority="core">randomForest</pkg>
    <pkg>randomSurvivalForest</pkg>
    <pkg>rdetools</pkg>
    <pkg>relaxo</pkg>
    <pkg>rgenoud</pkg>
    <pkg priority="core">rpart</pkg>
    <pkg>RWeka</pkg>
    <pkg>svmpath</pkg>
    <pkg>tgp</pkg>
    <pkg>tree</pkg>
    <pkg>varSelRF</pkg>
    <pkg priority="core">VR</pkg>
  </packagelist>

  <links>
    <a href="http://www.MLOSS.org/">MLOSS: Machine Learning Open Source Software</a>
    <a href="http://www.boosting.org/">Boosting Research Site</a>
  </links>

</CRANTaskView>
